{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Préparation, Visualisation et Évaluation des Données\n",
    "\n",
    "## Objectif\n",
    "Ce notebook guide à travers les étapes de collecte de données brutes pour des paires et timeframes spécifiés, leur chargement, fusion, l'enrichissement avec des features techniques (y compris HMM et potentiellement CryptoBERT), la visualisation du dataset résultant, et sa sauvegarde pour les étapes suivantes (entraînement, backtesting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Globale du Notebook\n",
    "\n",
    "Modifiez les variables dans la cellule suivante pour configurer le notebook selon vos besoins (paires, dates, clés API, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,123 - __main__ - INFO - Modules projet importés.\n",
      "2025-05-20 11:56:55,124 - __main__ - INFO - Instance de Config créée. Project root utilisé par Config: /home/morningstar/Desktop/crypto_robot/Morningstar\n",
      "2025-05-20 11:56:55,126 - __main__ - INFO - Variable d'env 'BINANCE_API_KEY' définie pour cette session.\n",
      "2025-05-20 11:56:55,127 - __main__ - INFO - Variable d'env 'BINANCE_API_SECRET' définie pour cette session.\n",
      "2025-05-20 11:56:55,129 - __main__ - INFO - Variable d'env 'BITGET_API_KEY' définie pour cette session.\n",
      "2025-05-20 11:56:55,129 - __main__ - INFO - Variable d'env 'BITGET_API_SECRET' définie pour cette session.\n",
      "2025-05-20 11:56:55,130 - __main__ - INFO - Variable d'env 'BITGET_PASSPHRASE' définie pour cette session.\n",
      "2025-05-20 11:56:55,131 - __main__ - INFO - Variable d'env 'GOOGLE_API_KEY' définie pour cette session.\n",
      "2025-05-20 11:56:55,132 - __main__ - INFO - Variable d'env 'GOOGLE_CSE_ID' définie pour cette session.\n",
      "2025-05-20 11:56:55,133 - __main__ - INFO - Variable d'env 'GEMINI_API_KEY_1' définie pour cette session.\n",
      "2025-05-20 11:56:55,134 - __main__ - INFO - Variable d'env 'OPENROUTER_API_KEY' définie pour cette session.\n",
      "2025-05-20 11:56:55,135 - __main__ - INFO - Configuration du notebook terminée. Paires: ['BTC/USDT', 'XRP/USDT', 'BNB/USDT', 'SHIB/USDT', 'MATIC/USDT'], Timeframes: ['1m'], Période: 2020-10-01-2024-11-01\n"
     ]
    }
   ],
   "source": [
    "# --- Initialisation des Modules Python (Obligatoire en premier) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# --- Paramètres de Configuration Principaux (À MODIFIER PAR L'UTILISATEUR) ---\n",
    "TARGET_PAIRS_INPUT = [\"BTC/USDT\", \"XRP/USDT\", \"BNB/USDT\", \"SHIB/USDT\", \"MATIC/USDT\"]  # Liste ou string CSV\n",
    "TARGET_TIMEFRAMES_INPUT = \"1m\" # String ou liste\n",
    "START_DATE_INPUT = \"2020-10-01\"\n",
    "END_DATE_INPUT = \"2024-11-01\"\n",
    "EXCHANGE_ID_FOR_COLLECTION_INPUT = \"binance\"\n",
    "\n",
    "# --- Booléens pour activer/désactiver des sections ---\n",
    "RUN_DATA_COLLECTION = True\n",
    "LOAD_FIXTURE_IF_EMPTY = True\n",
    "RUN_FEATURE_ENGINEERING = True\n",
    "RUN_DATA_VALIDATION = True\n",
    "\n",
    "# --- Configuration des Clés API ---\n",
    "API_KEYS_INPUT = {\n",
    "    \"BINANCE_API_KEY\": \"VOTRE_BINANCE_KEY_ICI_SI_NON_DEFINIE_EN_ENV\",\n",
    "    \"BINANCE_API_SECRET\": \"VOTRE_BINANCE_SECRET_ICI_SI_NON_DEFINI_EN_ENV\",\n",
    "    \"BITGET_API_KEY\": \"VOTRE_BITGET_KEY_ICI_SI_NON_DEFINIE_EN_ENV\",\n",
    "    \"BITGET_API_SECRET\": \"VOTRE_BITGET_SECRET_ICI_SI_NON_DEFINI_EN_ENV\",\n",
    "    \"BITGET_PASSPHRASE\": \"VOTRE_BITGET_PASSPHRASE_ICI_SI_NON_DEFINI_EN_ENV\",\n",
    "    \"GOOGLE_API_KEY\": \"VOTRE_GOOGLE_KEY_ICI_SI_NON_DEFINIE_EN_ENV\",\n",
    "    \"GOOGLE_CSE_ID\": \"VOTRE_GOOGLE_CSE_ID_ICI_SI_NON_DEFINI_EN_ENV\",\n",
    "    \"GEMINI_API_KEY_1\": \"VOTRE_GEMINI_KEY_ICI_SI_NON_DEFINIE_EN_ENV\",\n",
    "    \"OPENROUTER_API_KEY\": \"VOTRE_OPENROUTER_KEY_ICI_SI_NON_DEFINI_EN_ENV\"\n",
    "}\n",
    "\n",
    "# --- Chemins ---\n",
    "PROJECT_ROOT_NOTEBOOK_LEVEL = os.path.abspath(os.getcwd())\n",
    "PROJECT_CODE_ROOT = os.path.join(PROJECT_ROOT_NOTEBOOK_LEVEL, \"ultimate\")\n",
    "RAW_DATA_DIR_NAME = \"data/raw/market\"\n",
    "PROCESSED_DATA_DIR_NAME = \"data/processed\"\n",
    "PROCESSED_DATA_FILENAME = \"multipaire.parquet\"\n",
    "\n",
    "# --- Initialisation (Logging, PYTHONPATH, Config, Variables Globales) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "sns.set_theme(style=\"whitegrid\"); plt.rcParams['figure.figsize'] = (18, 6); plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "if PROJECT_CODE_ROOT not in sys.path: sys.path.append(PROJECT_CODE_ROOT); logger.info(f\"'{PROJECT_CODE_ROOT}' ajouté au PYTHONPATH.\")\n",
    "\n",
    "cfg_instance = None; apply_feature_pipeline = None; load_and_split_data = None\n",
    "try:\n",
    "    from utils.feature_engineering import apply_feature_pipeline as apply_feature_pipeline_imported\n",
    "    apply_feature_pipeline = apply_feature_pipeline_imported\n",
    "    from model.training.data_loader import load_and_split_data as load_and_split_data_imported\n",
    "    load_and_split_data = load_and_split_data_imported\n",
    "    from config.config import Config\n",
    "    logger.info(\"Modules projet importés.\")\n",
    "    cfg_instance = Config()\n",
    "    logger.info(f\"Instance de Config créée. Project root utilisé par Config: {getattr(cfg_instance, '_project_root', 'Non défini par la classe Config')}\")\n",
    "    if not cfg_instance.yaml_config:\n",
    "        logger.error(\"CRITIQUE: config.yaml non chargé par Config(). Assurez-vous qu'il est dans 'Morningstar/config/config.yaml' et que la classe Config le trouve.\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\"Erreur import modules projet: {e}. Certaines fonctionnalités seront désactivées.\", exc_info=True)\n",
    "    if cfg_instance is None:\n",
    "        class FallbackConfig:\n",
    "            _project_root = PROJECT_ROOT_NOTEBOOK_LEVEL\n",
    "            yaml_config = {}\n",
    "            def get_config(self, key, default=None): return default\n",
    "        cfg_instance = FallbackConfig()\n",
    "        logger.warning(\"Utilisation d'une instance Config de fallback suite à une erreur d'import.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erreur majeure lors de l'initialisation de Config: {e}. Utilisation de FallbackConfig.\", exc_info=True)\n",
    "    if cfg_instance is None:\n",
    "        class FallbackConfig:\n",
    "            _project_root = PROJECT_ROOT_NOTEBOOK_LEVEL\n",
    "            yaml_config = {}\n",
    "            def get_config(self, key, default=None): return default\n",
    "        cfg_instance = FallbackConfig()\n",
    "        logger.warning(\"Utilisation d'une instance Config de fallback suite à une erreur majeure.\")\n",
    "\n",
    "# --- Correction robuste pour TARGET_PAIRS et TARGET_TIMEFRAMES ---\n",
    "def ensure_list(val):\n",
    "    if isinstance(val, str):\n",
    "        return [v.strip() for v in val.split(',') if v.strip()]\n",
    "    elif isinstance(val, list):\n",
    "        return [str(v).strip() for v in val if str(v).strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "TARGET_PAIRS = ensure_list(TARGET_PAIRS_INPUT)\n",
    "TARGET_TIMEFRAMES = ensure_list(TARGET_TIMEFRAMES_INPUT)\n",
    "START_DATE = START_DATE_INPUT\n",
    "END_DATE = END_DATE_INPUT\n",
    "EXCHANGE_ID_FOR_COLLECTION = EXCHANGE_ID_FOR_COLLECTION_INPUT\n",
    "\n",
    "RAW_DATA_DIR = Path(PROJECT_CODE_ROOT) / RAW_DATA_DIR_NAME\n",
    "PROCESSED_DATA_DIR = Path(PROJECT_CODE_ROOT) / PROCESSED_DATA_DIR_NAME\n",
    "PROCESSED_DATA_OUTPUT_PATH = PROCESSED_DATA_DIR / PROCESSED_DATA_FILENAME\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for key, value in API_KEYS_INPUT.items():\n",
    "    env_value = os.getenv(key)\n",
    "    if \"_A_COLLER_ICI_SI_NON_DEFINIE_EN_ENV\" not in value and value.strip() != \"\":\n",
    "        os.environ[key] = value; logger.info(f\"Variable d'env '{key}' définie pour cette session.\")\n",
    "    elif not env_value:\n",
    "        logger.warning(f\"Clé API '{key}' non définie.\")\n",
    "    else:\n",
    "        logger.info(f\"Clé API '{key}' utilisée depuis l'environnement.\")\n",
    "\n",
    "logger.info(f\"Configuration du notebook terminée. Paires: {TARGET_PAIRS}, Timeframes: {TARGET_TIMEFRAMES}, Période: {START_DATE}-{END_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collecte des Données Brutes (Optionnel)\n",
    "\n",
    "Cette section exécute le script `ultimate/data_collectors/market_data_collector.py` si `RUN_DATA_COLLECTION` est `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,156 - __main__ - INFO - Collecte de données brutes sautée (RUN_DATA_COLLECTION=False). Le notebook tentera de charger des fichiers existants.\n"
     ]
    }
   ],
   "source": [
    "if RUN_DATA_COLLECTION:\n",
    "    logger.info(\"Début de la collecte de données brutes via market_data_collector.py...\")\n",
    "    collector_script_path = os.path.join(PROJECT_CODE_ROOT, \"data_collectors\", \"market_data_collector.py\")\n",
    "    \n",
    "    if not os.path.exists(collector_script_path):\n",
    "        logger.error(f\"Script de collecte non trouvé: {collector_script_path}. Vérifiez le chemin.\")\n",
    "    elif not TARGET_PAIRS or not TARGET_TIMEFRAMES:\n",
    "        logger.error(\"TARGET_PAIRS ou TARGET_TIMEFRAMES non définis. Collecte annulée.\")\n",
    "    else:\n",
    "        pairs_str = \",\".join(TARGET_PAIRS)\n",
    "        timeframes_str = \",\".join(TARGET_TIMEFRAMES)\n",
    "        command = [\n",
    "            sys.executable, collector_script_path,\n",
    "            \"--exchange\", EXCHANGE_ID_FOR_COLLECTION,\n",
    "            \"--pairs\", pairs_str, \"--timeframes\", timeframes_str,\n",
    "            \"--start-date\", START_DATE, \"--end-date\", END_DATE,\n",
    "            \"--output-dir\", str(RAW_DATA_DIR)\n",
    "        ]\n",
    "        logger.info(f\"Exécution de la commande de collecte: {' '.join(command)}\")\n",
    "        try:\n",
    "            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, cwd=PROJECT_CODE_ROOT)\n",
    "            stdout, stderr = process.communicate(timeout=1800) # Timeout de 30 minutes\n",
    "            \n",
    "            logger.info(\"--- Sortie Standard du Script de Collecte ---\")\n",
    "            for line in stdout.splitlines(): logger.info(f\"[COLLECTOR] {line}\")\n",
    "            logger.info(\"--- Fin Sortie Standard ---\")\n",
    "            \n",
    "            if process.returncode == 0:\n",
    "                logger.info(\"Collecte de données terminée avec succès.\")\n",
    "            else:\n",
    "                logger.error(f\"Le script de collecte de données a échoué avec le code de retour {process.returncode}.\")\n",
    "            if stderr:\n",
    "                logger.error(\"--- Erreurs du Script de Collecte ---\")\n",
    "                for line in stderr.splitlines(): logger.error(f\"[COLLECTOR_ERR] {line}\")\n",
    "                logger.error(\"--- Fin Erreurs Collecte ---\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            logger.error(\"La collecte de données a dépassé le délai de 30 minutes.\", exc_info=True)\n",
    "            if 'process' in locals() and process.poll() is None: process.kill()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'exécution du script de collecte: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.info(\"Collecte de données brutes sautée (RUN_DATA_COLLECTION=False). Le notebook tentera de charger des fichiers existants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement et Fusion des Données Brutes\n",
    "\n",
    "Charge les fichiers Parquet individuels (un par paire/timeframe) depuis `ultimate/data/raw/market/` et les fusionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,192 - __main__ - INFO - Chargement des fichiers Parquet depuis /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/data/raw/market pour ['BTC/USDT', 'XRP/USDT', 'BNB/USDT', 'SHIB/USDT', 'MATIC/USDT'] et ['1m']...\n",
      "2025-05-20 11:56:55,197 - __main__ - WARNING - Fichier non trouvé: /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/data/raw/market/BTCUSDT_1m.parquet.\n",
      "2025-05-20 11:56:55,198 - __main__ - WARNING - Fichier non trouvé: /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/data/raw/market/XRPUSDT_1m.parquet.\n",
      "2025-05-20 11:56:55,200 - __main__ - WARNING - Fichier non trouvé: /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/data/raw/market/BNBUSDT_1m.parquet.\n",
      "2025-05-20 11:56:55,200 - __main__ - WARNING - Fichier non trouvé: /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/data/raw/market/SHIBUSDT_1m.parquet.\n",
      "2025-05-20 11:56:55,201 - __main__ - WARNING - Fichier non trouvé: /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/data/raw/market/MATICUSDT_1m.parquet.\n",
      "2025-05-20 11:56:55,202 - __main__ - WARNING - Aucune donnée brute chargée pour les cibles.\n",
      "2025-05-20 11:56:55,203 - __main__ - WARNING - Utilisation du fichier de fixture car aucune donnée cible n'a été chargée ou la fusion a échoué.\n",
      "2025-05-20 11:56:55,204 - __main__ - ERROR - Fichier de fixture /home/morningstar/Desktop/crypto_robot/Morningstar/ultimate/tests/fixtures/golden_backtest.parquet non trouvé. Le DataFrame restera vide.\n"
     ]
    }
   ],
   "source": [
    "all_loaded_dfs = []\n",
    "df_raw_combined = pd.DataFrame() \n",
    "logger.info(f\"Chargement des fichiers Parquet depuis {RAW_DATA_DIR} pour {TARGET_PAIRS} et {TARGET_TIMEFRAMES}...\")\n",
    "\n",
    "if TARGET_PAIRS and TARGET_TIMEFRAMES and isinstance(TARGET_PAIRS, list) and isinstance(TARGET_TIMEFRAMES, list):\n",
    "    for pair in TARGET_PAIRS:\n",
    "        for tf in TARGET_TIMEFRAMES:\n",
    "            safe_pair_name = pair.replace('/', '')\n",
    "            expected_filename = f\"{safe_pair_name}_{tf}.parquet\"\n",
    "            file_path = RAW_DATA_DIR / expected_filename\n",
    "            if file_path.exists():\n",
    "                logger.info(f\"Chargement de {file_path}...\")\n",
    "                try:\n",
    "                    df_temp = pd.read_parquet(file_path)\n",
    "                    df_temp['pair'] = pair\n",
    "                    df_temp['timeframe'] = tf\n",
    "                    if 'timestamp' in df_temp.columns:\n",
    "                        df_temp['timestamp'] = pd.to_datetime(df_temp['timestamp'], errors='coerce', utc=True)\n",
    "                    elif isinstance(df_temp.index, pd.DatetimeIndex):\n",
    "                         df_temp = df_temp.reset_index().rename(columns={'index':'timestamp'})\n",
    "                         df_temp['timestamp'] = pd.to_datetime(df_temp['timestamp'], errors='coerce', utc=True)\n",
    "                    else:\n",
    "                        logger.error(f\"Colonne 'timestamp' ou index Datetime manquant dans {file_path}. Fichier ignoré.\")\n",
    "                        continue \n",
    "                    df_temp.dropna(subset=['timestamp'], inplace=True) \n",
    "                    if not df_temp.empty:\n",
    "                        all_loaded_dfs.append(df_temp)\n",
    "                        logger.info(f\"Chargé {file_path}. Shape: {df_temp.shape}. Colonnes: {df_temp.columns.tolist()[:5]}...\")\n",
    "                    else:\n",
    "                        logger.warning(f\"DataFrame vide après traitement de timestamp pour {file_path}.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erreur chargement {file_path}: {e}\", exc_info=True)\n",
    "            else:\n",
    "                logger.warning(f\"Fichier non trouvé: {file_path}.\")\n",
    "else:\n",
    "    logger.warning(\"TARGET_PAIRS ou TARGET_TIMEFRAMES non définis correctement.\")\n",
    "\n",
    "if all_loaded_dfs:\n",
    "    try:\n",
    "        df_raw_combined = pd.concat(all_loaded_dfs, ignore_index=True)\n",
    "        if 'timestamp' in df_raw_combined.columns and not df_raw_combined.empty:\n",
    "            df_raw_combined = df_raw_combined.sort_values(by=['pair', 'timeframe', 'timestamp']).reset_index(drop=True)\n",
    "            logger.info(f\"Données brutes fusionnées et triées. Shape finale: {df_raw_combined.shape}\")\n",
    "            print(\"\\n--- Aperçu des Données Brutes Combinées (df_raw_combined) ---\")\n",
    "            print(df_raw_combined.head())\n",
    "            print(\"\\n--- Statistiques par Paire/Timeframe ---\")\n",
    "            print(df_raw_combined.groupby(['pair', 'timeframe']).size().reset_index(name='counts'))\n",
    "            if not df_raw_combined.empty:\n",
    "                logger.info(\"\\n--- Graphiques des Prix de Clôture (Bruts) par Paire/Timeframe ---\")\n",
    "                for (pair_val, tf_val), group in df_raw_combined.groupby(['pair', 'timeframe']):\n",
    "                    plt.figure(figsize=(18,4))\n",
    "                    group.set_index('timestamp')['close'].plot(title=f'{pair_val} - {tf_val} - Prix de Clôture (Brut)')\n",
    "                    plt.ylabel('Prix de Clôture'); plt.xlabel('Timestamp'); plt.grid(True); plt.tight_layout()\n",
    "                    plt.show()\n",
    "        elif df_raw_combined.empty:\n",
    "            logger.warning(\"df_raw_combined est vide après tentative de fusion.\")\n",
    "        else: \n",
    "            logger.error(\"Colonne 'timestamp' manquante après fusion. df_raw_combined invalidé.\")\n",
    "            df_raw_combined = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur concaténation/tri: {e}\", exc_info=True)\n",
    "        df_raw_combined = pd.DataFrame()\n",
    "else:\n",
    "    logger.warning(\"Aucune donnée brute chargée pour les cibles.\")\n",
    "\n",
    "if df_raw_combined.empty and LOAD_FIXTURE_IF_EMPTY:\n",
    "    logger.warning(\"Utilisation du fichier de fixture car aucune donnée cible n'a été chargée ou la fusion a échoué.\")\n",
    "    fixture_path = os.path.join(PROJECT_CODE_ROOT, \"tests\", \"fixtures\", \"golden_backtest.parquet\")\n",
    "    if os.path.exists(fixture_path):\n",
    "        try:\n",
    "            df_raw_combined = pd.read_parquet(fixture_path)\n",
    "            if 'pair' not in df_raw_combined.columns: df_raw_combined['pair'] = (TARGET_PAIRS[0] if (TARGET_PAIRS and isinstance(TARGET_PAIRS, list)) else 'BTC/USDT')\n",
    "            if 'timeframe' not in df_raw_combined.columns: df_raw_combined['timeframe'] = (TARGET_TIMEFRAMES[0] if (TARGET_TIMEFRAMES and isinstance(TARGET_TIMEFRAMES, list)) else '1h')\n",
    "            if 'timestamp' in df_raw_combined.columns: df_raw_combined['timestamp'] = pd.to_datetime(df_raw_combined['timestamp'], utc=True)\n",
    "            logger.info(f\"Données de fixture chargées. Shape: {df_raw_combined.shape}\")\n",
    "            print(df_raw_combined.head())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur chargement fixture {fixture_path}: {e}\")\n",
    "            df_raw_combined = pd.DataFrame()\n",
    "    else:\n",
    "        logger.error(f\"Fichier de fixture {fixture_path} non trouvé. Le DataFrame restera vide.\")\n",
    "elif df_raw_combined.empty and not LOAD_FIXTURE_IF_EMPTY:\n",
    "    logger.error(\"Aucune donnée chargée et chargement de fixture désactivé. Le notebook ne peut pas continuer sans données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enrichissement & Calcul des Features\n",
    "\n",
    "Applique `apply_feature_pipeline` (de `ultimate/utils/feature_engineering.py`) sur le DataFrame combiné.\n",
    "Cette fonction est responsable de calculer tous les indicateurs techniques, les features HMM, et potentiellement les features LLM (CryptoBERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,242 - __main__ - WARNING - Enrichissement sauté car df_raw_combined est vide ou non chargé.\n"
     ]
    }
   ],
   "source": [
    "df_feat = pd.DataFrame() \n",
    "if RUN_FEATURE_ENGINEERING:\n",
    "    if df_raw_combined is not None and not df_raw_combined.empty:\n",
    "        logger.info(\"Début de l'enrichissement des données et du calcul des features...\")\n",
    "        try:\n",
    "            if 'apply_feature_pipeline' in globals() and callable(apply_feature_pipeline):\n",
    "                if 'pair' in df_raw_combined.columns and 'timeframe' in df_raw_combined.columns and df_raw_combined[['pair', 'timeframe']].nunique().prod() > 1:\n",
    "                    logger.info(\"Application du pipeline de features par groupe (paire, timeframe)...\")\n",
    "                    df_feat_list = []\n",
    "                    for name_tuple, group_df in df_raw_combined.groupby(['pair', 'timeframe']):\n",
    "                        logger.info(f\"Traitement des features pour {name_tuple} (Shape: {group_df.shape})...\")\n",
    "                        group_for_feat = group_df.copy()\n",
    "                        if 'timestamp' in group_for_feat.columns:\n",
    "                            group_for_feat = group_for_feat.set_index('timestamp') \n",
    "                        elif not isinstance(group_for_feat.index, pd.DatetimeIndex):\n",
    "                            logger.error(f\"Groupe {name_tuple} sans index timestamp valide. Features non calculées.\")\n",
    "                            continue\n",
    "                        processed_group = apply_feature_pipeline(group_for_feat) \n",
    "                        if processed_group is not None and not processed_group.empty:\n",
    "                            processed_group['pair'] = name_tuple[0]\n",
    "                            processed_group['timeframe'] = name_tuple[1]\n",
    "                            df_feat_list.append(processed_group.reset_index()) \n",
    "                        else:\n",
    "                            logger.warning(f\"apply_feature_pipeline a retourné None/vide pour {name_tuple}.\")\n",
    "                    if df_feat_list:\n",
    "                        df_feat = pd.concat(df_feat_list, ignore_index=True)\n",
    "                        if 'timestamp' in df_feat.columns: \n",
    "                            df_feat = df_feat.sort_values(by=['pair', 'timeframe', 'timestamp']).reset_index(drop=True)\n",
    "                else: \n",
    "                    logger.info(\"Application du pipeline de features sur le DataFrame entier...\")\n",
    "                    df_temp_for_feat = df_raw_combined.copy()\n",
    "                    if 'timestamp' in df_temp_for_feat.columns:\n",
    "                         df_temp_for_feat = df_temp_for_feat.set_index('timestamp')\n",
    "                    elif not isinstance(df_temp_for_feat.index, pd.DatetimeIndex):\n",
    "                        logger.error(\"DataFrame n'a pas d'index timestamp valide. Features non calculées.\")\n",
    "                        df_temp_for_feat = None\n",
    "                    if df_temp_for_feat is not None:\n",
    "                         df_feat = apply_feature_pipeline(df_temp_for_feat)\n",
    "                         if df_feat is not None: \n",
    "                             df_feat = df_feat.reset_index() \n",
    "                             if 'pair' not in df_feat.columns and 'pair' in df_raw_combined.columns: df_feat['pair'] = df_raw_combined['pair'].iloc[0] if len(df_raw_combined['pair'].unique()) == 1 else 'MULTI_PAIR'\n",
    "                             if 'timeframe' not in df_feat.columns and 'timeframe' in df_raw_combined.columns: df_feat['timeframe'] = df_raw_combined['timeframe'].iloc[0] if len(df_raw_combined['timeframe'].unique()) == 1 else 'MULTI_TF'\n",
    "            else:\n",
    "                logger.error(\"La fonction `apply_feature_pipeline` n'a pas été importée ou n'est pas callable. Enrichissement sauté.\")\n",
    "            \n",
    "            if df_feat is not None and not df_feat.empty:\n",
    "                logger.info(f\"Dataset enrichi. Shape finale: {df_feat.shape}\")\n",
    "                print(\"\\n--- Aperçu du Dataset Enrichi (df_feat) ---\")\n",
    "                print(df_feat.head())\n",
    "                logger.info(f\"Colonnes de df_feat: {df_feat.columns.tolist()}\")\n",
    "            else:\n",
    "                logger.error(\"L'application du pipeline de features a résulté en un DataFrame vide ou None.\")\n",
    "                df_feat = pd.DataFrame() \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de apply_feature_pipeline: {e}\", exc_info=True)\n",
    "            df_feat = pd.DataFrame()\n",
    "    else:\n",
    "        logger.warning(\"Enrichissement sauté car df_raw_combined est vide ou non chargé.\")\n",
    "else:\n",
    "    logger.info(\"Calcul des features désactivé (RUN_FEATURE_ENGINEERING=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vérification Détaillée des Features et Visualisations Post-Enrichissement\n",
    "\n",
    "Cette section analyse le DataFrame `df_feat` après l'application du pipeline de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,290 - __main__ - WARNING - Vérification détaillée et visualisation des features sautées car df_feat est vide.\n"
     ]
    }
   ],
   "source": [
    "if RUN_DATA_VALIDATION and df_feat is not None and not df_feat.empty:\n",
    "    logger.info(\"\\n--- Vérification Détaillée et Visualisation des Features ---\")\n",
    "    print(f\"Shape de df_feat: {df_feat.shape}\")\n",
    "    print(f\"Nombre total de colonnes dans df_feat: {len(df_feat.columns)}\")\n",
    "    print(f\"Colonnes présentes: {df_feat.columns.tolist()}\")\n",
    "\n",
    "    # Vérification des NaNs\n",
    "    nan_counts = df_feat.isnull().sum()\n",
    "    nan_percent = (nan_counts / len(df_feat)) * 100\n",
    "    nan_summary = pd.DataFrame({'NaN_Count': nan_counts, 'NaN_Percent': nan_percent})\n",
    "    print(\"\\n--- Résumé des Valeurs Manquantes (NaN) par Colonne ---\")\n",
    "    print(nan_summary[nan_summary['NaN_Count'] > 0].sort_values(by='NaN_Percent', ascending=False))\n",
    "\n",
    "    # Vérification CryptoBERT\n",
    "    bert_cols = [col for col in df_feat.columns if col.startswith('bert_')]\n",
    "    if bert_cols:\n",
    "        logger.info(f\"{len(bert_cols)} features CryptoBERT trouvées.\")\n",
    "        nan_bert_percent = df_feat[bert_cols].isnull().sum().sum() / (df_feat[bert_cols].size if df_feat[bert_cols].size > 0 else 1) * 100\n",
    "        logger.info(f\"Pourcentage de NaN dans features CryptoBERT: {nan_bert_percent:.2f}%\")\n",
    "        if nan_bert_percent > 50:\n",
    "            logger.warning(\"Beaucoup de NaNs dans les features CryptoBERT. Vérifiez la source de données textuelles.\")\n",
    "    else:\n",
    "        logger.warning(\"Aucune feature CryptoBERT ('bert_*') trouvée.\")\n",
    "\n",
    "    # Vérification HMM\n",
    "    hmm_regime_col = 'hmm_regime'\n",
    "    hmm_prob_cols = [col for col in df_feat.columns if col.startswith('hmm_prob_')]\n",
    "    if hmm_regime_col in df_feat.columns:\n",
    "        logger.info(f\"Feature HMM '{hmm_regime_col}' trouvée. Distribution:\")\n",
    "        if 'pair' in df_feat.columns and 'timeframe' in df_feat.columns:\n",
    "            try:\n",
    "                print(df_feat.groupby(['pair', 'timeframe'])[hmm_regime_col].value_counts(normalize=True, dropna=False).unstack(fill_value=0).round(3))\n",
    "                if df_feat[hmm_regime_col].nunique() > 1: \n",
    "                    g = sns.catplot(data=df_feat, x=hmm_regime_col, col='pair', row='timeframe', kind='count', sharey=False, height=3.5, aspect=1.2, palette='viridis')\n",
    "                    g.fig.suptitle('Distribution des Régimes HMM par Paire/Timeframe', y=1.03); plt.tight_layout(); plt.show()\n",
    "                else:\n",
    "                    logger.info(f\"Une seule valeur unique pour {hmm_regime_col}, graphique countplot non affiché.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Erreur visualisation régimes HMM: {e}\")\n",
    "        else:\n",
    "            print(df_feat[hmm_regime_col].value_counts(normalize=True, dropna=False))\n",
    "    else:\n",
    "        logger.warning(f\"Feature HMM '{hmm_regime_col}' non trouvée. C'est une colonne de label importante.\")\n",
    "    if hmm_prob_cols:\n",
    "        logger.info(f\"{len(hmm_prob_cols)} features de probabilité HMM trouvées.\")\n",
    "        nan_hmm_prob_percent = df_feat[hmm_prob_cols].isnull().sum().sum() / (df_feat[hmm_prob_cols].size if df_feat[hmm_prob_cols].size > 0 else 1) * 100\n",
    "        logger.info(f\"Pourcentage de NaN dans probabilités HMM: {nan_hmm_prob_percent:.2f}%\")\n",
    "    else:\n",
    "        logger.warning(\"Aucune feature de probabilité HMM ('hmm_prob_*') trouvée.\")\n",
    "\n",
    "    # Visualisation de features techniques clés\n",
    "    key_tech_features = [col for col in ['close', 'feature_SMA_10', 'feature_RSI_14', 'MACD', 'BBM', 'ATR', hmm_regime_col] if col in df_feat.columns]\n",
    "    if len(key_tech_features) > 1 and 'timestamp' in df_feat.columns and 'pair' in df_feat.columns and 'timeframe' in df_feat.columns:\n",
    "        logger.info(f\"Visualisation de features techniques clés: {key_tech_features}\")\n",
    "        for (pair_val, tf_val), group in df_feat.groupby(['pair', 'timeframe']):\n",
    "            plot_df = group.set_index('timestamp')\n",
    "            actual_cols_to_plot = [col for col in key_tech_features if col in plot_df.columns]\n",
    "            if len(actual_cols_to_plot) > 0:\n",
    "                plot_df[actual_cols_to_plot].plot(subplots=True, figsize=(18, 2.5*len(actual_cols_to_plot)), layout=(-1,1), sharex=True, title=f'Features Clés pour {pair_val} - {tf_val}')\n",
    "                plt.tight_layout(); plt.show()\n",
    "            else:\n",
    "                logger.warning(f\"Aucune des features {key_tech_features} à plotter pour {pair_val} - {tf_val}\")\n",
    "\n",
    "        # Matrice de corrélation\n",
    "        if df_feat['pair'].nunique() > 0 and df_feat['timeframe'].nunique() > 0:\n",
    "            grouped_for_corr = df_feat.groupby(['pair', 'timeframe'])\n",
    "            if grouped_for_corr.groups:\n",
    "                first_group_key = list(grouped_for_corr.groups.keys())[0]\n",
    "                first_group_df = grouped_for_corr.get_group(first_group_key)\n",
    "                cols_for_corr = [col for col in ['close', 'volume', 'feature_RSI_14', 'MACD', 'ATR', hmm_regime_col] if col in first_group_df.columns and first_group_df[col].dtype in [np.float64, np.int64]]\n",
    "                if len(cols_for_corr) > 1:\n",
    "                    plt.figure(figsize=(10, 8))\n",
    "                    correlation_matrix = first_group_df[cols_for_corr].corr()\n",
    "                    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "                    plt.title(f'Matrice de Corrélation pour {first_group_key}')\n",
    "                    plt.show()\n",
    "            else:\n",
    "                logger.warning(\"Aucun groupe trouvé pour la matrice de corrélation.\")\n",
    "    else:\n",
    "        logger.warning(f\"Pas assez de features/colonnes pour visualisation détaillée.\")\n",
    "elif RUN_DATA_VALIDATION:\n",
    "    logger.warning(\"Vérification détaillée et visualisation des features sautées car df_feat est vide.\")\n",
    "else:\n",
    "    logger.info(\"Vérification détaillée et visualisation des features désactivées (RUN_DATA_VALIDATION=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export du Dataset Enrichi\n",
    "\n",
    "Sauvegarde de `df_feat` dans `PROCESSED_DATA_OUTPUT_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,322 - __main__ - WARNING - Export sauté car df_feat est vide ou non chargé.\n"
     ]
    }
   ],
   "source": [
    "# --- Génération automatique de 'market_regime' si manquante ---\n",
    "if df_feat is not None and not df_feat.empty:\n",
    "    if 'market_regime' not in df_feat.columns:\n",
    "        logger.warning(\"La colonne 'market_regime' est manquante — génération automatique à partir de la variation de 'close'.\")\n",
    "        df_feat['market_regime'] = (df_feat['close'].diff() > 0).astype(int).fillna(0).astype(int)\n",
    "        logger.info(f\"'market_regime' générée, distribution :\\n{df_feat['market_regime'].value_counts(dropna=False)}\")\n",
    "\n",
    "    # --- Sauvegarde du dataset enrichi ---\n",
    "    try:\n",
    "        df_feat.to_parquet(PROCESSED_DATA_OUTPUT_PATH, index=False)\n",
    "        logger.info(f\"Dataset enrichi sauvegardé dans {PROCESSED_DATA_OUTPUT_PATH}\")\n",
    "        # Relecture pour vérification\n",
    "        df_check = pd.read_parquet(PROCESSED_DATA_OUTPUT_PATH)\n",
    "        exists = 'market_regime' in df_check.columns\n",
    "        logger.info(f\"'market_regime' présente après relecture ? {exists}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l’export / vérification : {e}\", exc_info=True)\n",
    "        raise\n",
    "else:\n",
    "    logger.warning(\"Export sauté car df_feat est vide ou non chargé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vérification de la Compatibilité pour l'Entraînement\n",
    "\n",
    "Utilise `load_and_split_data` pour s'assurer que le dataset sauvegardé peut être chargé et contient les colonnes de labels attendues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 11:56:55,344 - __main__ - WARNING - Export sauté car df_feat est vide ou non chargé.\n"
     ]
    }
   ],
   "source": [
    "# --- Cellule 6 : Vérification et sauvegarde du dataset enrichi ---\n",
    "if df_feat is not None and not df_feat.empty:\n",
    "    # 1. Génération automatique de 'market_regime' si manquante\n",
    "    if 'market_regime' not in df_feat.columns:\n",
    "        logger.warning(\"La colonne 'market_regime' est manquante — génération automatique à partir de la variation de 'close'.\")\n",
    "        # Simple rule-based regime: 1 si la clôture monte par rapport à la précédente, sinon 0\n",
    "        df_feat['market_regime'] = (df_feat['close'].diff() > 0).astype(int).fillna(0).astype(int)\n",
    "        logger.info(f\"'market_regime' générée, distribution :\\n{df_feat['market_regime'].value_counts(dropna=False)}\")\n",
    "\n",
    "    # 2. Sauvegarde\n",
    "    try:\n",
    "        df_feat.to_parquet(PROCESSED_DATA_OUTPUT_PATH, index=False)\n",
    "        logger.info(f\"Dataset enrichi sauvegardé dans {PROCESSED_DATA_OUTPUT_PATH}\")\n",
    "        # relecture pour vérification\n",
    "        df_check = pd.read_parquet(PROCESSED_DATA_OUTPUT_PATH)\n",
    "        exists = 'market_regime' in df_check.columns\n",
    "        logger.info(f\"'market_regime' présente après relecture ? {exists}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l’export / vérification : {e}\", exc_info=True)\n",
    "        raise\n",
    "else:\n",
    "    logger.warning(\"Export sauté car df_feat est vide ou non chargé.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
